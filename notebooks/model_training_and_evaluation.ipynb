{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "633ad14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a528a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bdbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\" Random Seed fixed to {SEED} for reproducibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_processed.csv')\n",
    "test_df = pd.read_csv('../data/test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_clean'] = train_df['text_clean'].fillna(\"\")\n",
    "test_df['text_clean'] = test_df['text_clean'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "all_intents = pd.concat([train_df['intent_name'], test_df['intent_name']]).unique()\n",
    "label_encoder.fit(all_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_encoder.transform(train_df['intent_name'])\n",
    "y_test = label_encoder.transform(test_df['intent_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(label_encoder, '../models/label_encoder.pkl')\n",
    "\n",
    "print(f\" Data Loaded:\")\n",
    "print(f\"   - Training Samples: {len(train_df)}\")\n",
    "print(f\"   - Test Samples:     {len(test_df)}\")\n",
    "print(f\"   - Total Classes:    {len(label_encoder.classes_)}\")\n",
    "print(f\"   - Encoder saved to 'models/label_encoder.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Training Baseline Model (Logistic Regression)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2), \n",
    "    max_features=20000,  \n",
    "    stop_words='english' \n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['text_clean'])\n",
    "X_test_tfidf = tfidf.transform(test_df['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LogisticRegression(\n",
    "    class_weight='balanced', \n",
    "    random_state=SEED, \n",
    "    max_iter=1000, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = baseline_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_f1 = f1_score(y_test, y_pred_baseline, average='macro')\n",
    "print(f\"\\n Baseline Macro F1-Score: {baseline_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(baseline_model, '../models/baseline_model.pkl')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
    "print(\" Baseline Artifacts Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "print(\" Preparing Champion Model (DistilBERT)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(), \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=64, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'])\n",
    "test_encodings = tokenize_data(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BankingDataset(train_encodings, y_train)\n",
    "test_dataset = BankingDataset(test_encodings, y_test)\n",
    "\n",
    "print(\" Data Tokenized and Format Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d379ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",           \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,     \n",
    "    learning_rate=2e-5,              \n",
    "    use_cpu=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./results'):\n",
    "    checkpoints = [d for d in os.listdir('./results') if d.startswith('checkpoint')]\n",
    "else:\n",
    "    checkpoints = []\n",
    "\n",
    "if checkpoints:\n",
    "    \n",
    "    checkpoints.sort(key=lambda x: int(x.split('-')[1]))\n",
    "    latest_checkpoint = os.path.join('./results', checkpoints[-1])\n",
    "    print(f\" Found checkpoint: {latest_checkpoint}\")\n",
    "    print(\" Resuming training from there...\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "else:\n",
    "    print(\" No checkpoints found. Starting from scratch...\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0222b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Saving final model...\")\n",
    "model.save_pretrained(\"../models/distilbert_banking\")\n",
    "tokenizer.save_pretrained(\"../models/distilbert_banking\")\n",
    "print(\" Champion Model Saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Evaluating Champion Model...\")\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred_champion = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6235dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_f1 = f1_score(y_test, y_pred_champion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Baseline (LogReg)', 'Champion (DistilBERT)'],\n",
    "    'Macro F1': [baseline_f1, champion_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n FINAL RESULTS:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if champion_f1 > baseline_f1:\n",
    "    print(f\" Success: Champion beat Baseline by +{(champion_f1 - baseline_f1)*100:.2f}%\")\n",
    "else:\n",
    "    print(\" Warning: Champion did not outperform. Check hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_confusions(y_true, y_pred, labels, top_k=3):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    np.fill_diagonal(cm, 0) \n",
    "    \n",
    "    pairs = []\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            if cm[i, j] > 0:\n",
    "                pairs.append((\n",
    "                    labels[i], \n",
    "                    labels[j], \n",
    "                    cm[i, j]\n",
    "                ))\n",
    "    \n",
    "    \n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    return pairs[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7897092",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_confusions = get_top_confusions(y_test, y_pred_champion, label_encoder.classes_)\n",
    "\n",
    "print(\" Top 3 Most Confused Pairs (Champion Model):\")\n",
    "for true_label, pred_label, count in top_confusions:\n",
    "    print(f\"   - True: '{true_label}'  ->  Predicted: '{pred_label}' (Count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_champion), cmap='Blues')\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_intents = ['lost_or_stolen_card', 'compromised_card', 'lost_or_stolen_phone']\n",
    "risk_indices = label_encoder.transform(risk_intents)\n",
    "\n",
    "print(\" RISK AUDIT (Business Critical Check) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, intent in zip(risk_indices, risk_intents):\n",
    "    true_mask = (y_test == idx)\n",
    "    total_risk_samples = np.sum(true_mask)\n",
    "    \n",
    "    if total_risk_samples == 0:\n",
    "        continue\n",
    "        \n",
    "    correct_preds = np.sum((y_pred_champion == idx) & true_mask)\n",
    "    recall = correct_preds / total_risk_samples\n",
    "    \n",
    "    print(f\"\\nIntent: {intent}\")\n",
    "    print(f\"   - Total Cases: {total_risk_samples}\")\n",
    "    print(f\"   - Correctly Identified: {correct_preds}\")\n",
    "    print(f\"   - Recall Score: {recall:.2%}\")\n",
    "\n",
    "    if recall < 0.90:\n",
    "        print(\"    CRITICAL WARNING: Recall is below 90%. Manual review logic required.\")\n",
    "    else:\n",
    "        print(\"    Safety Check Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_predict(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    \n",
    "    risk_map = {\n",
    "        'compromised_card': ['hacked', 'compromised', 'unauthorized', 'suspicious'],\n",
    "        'lost_or_stolen_card': ['stolen', 'lost my card', 'robbed', 'missing']\n",
    "    }\n",
    "    \n",
    "    for intent, keywords in risk_map.items():\n",
    "        for word in keywords:\n",
    "            if word in text_lower:\n",
    "                return label_encoder.transform([intent])[0]\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "    model.to('cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Function loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Risk Test: {hybrid_predict('I was hacked')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Normal Test: {hybrid_predict('Hello bank')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd26131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Calculating Final Score...\")\n",
    "risk_indices = label_encoder.transform(['compromised_card'])\n",
    "risk_mask = (y_test == risk_indices[0]) \n",
    "total = np.sum(risk_mask)\n",
    "correct = 0\n",
    "\n",
    "for text in test_df[risk_mask]['text']:\n",
    "    if hybrid_predict(text) == risk_indices[0]:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Final Recall Score: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494557c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_predict(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    risk_map = {\n",
    "        'compromised_card': [\n",
    "            'hacked', 'compromised', 'unauthorized', 'suspicious', 'fraud', \n",
    "            'scam', 'phishing', 'fake', 'police', 'crime', 'victim',\n",
    "            \n",
    "         \n",
    "            'block', 'freeze', 'lock', 'stop', 'cancel', 'protect', \n",
    "            \n",
    "          \n",
    "            'didn\\'t make', 'did not make', 'wasn\\'t me', 'was not me',\n",
    "            'recognise', 'recognize', 'unknown', 'unfamiliar',\n",
    "            \n",
    "           \n",
    "            'details', 'pin', 'cvv', 'information', 'data', 'security'\n",
    "        ],\n",
    "        'lost_or_stolen_card': [\n",
    "            'stolen', 'lost', 'robbed', 'missing', 'dropped', 'gone', \n",
    "            'thief', 'theft', 'wallet', 'purse', 'bag'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for intent, keywords in risk_map.items():\n",
    "        for word in keywords:\n",
    "            if word in text_lower:\n",
    "                try:\n",
    "                    return label_encoder.transform([intent])[0]\n",
    "                except:\n",
    "                    continue \n",
    "\n",
    "   \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "    model.to('cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Calculating Final Score with Mega List...\")\n",
    "risk_indices = label_encoder.transform(['compromised_card'])\n",
    "risk_mask = (y_test == risk_indices[0]) \n",
    "total = np.sum(risk_mask)\n",
    "correct = 0\n",
    "\n",
    "print(\"--- Missed Cases (If Any) ---\")\n",
    "for text in test_df[risk_mask]['text']:\n",
    "    pred_id = hybrid_predict(text)\n",
    "    if pred_id == risk_indices[0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        \n",
    "        print(f\" MISSED: '{text}'\")\n",
    "\n",
    "print(f\"\\n Final Recall Score: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65dc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_predict(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    \n",
    "    risk_map = {\n",
    "        'compromised_card': [\n",
    "            \n",
    "            'hacked', 'compromised', 'unauthorized', 'suspicious', 'fraud', \n",
    "            'scam', 'phishing', 'fake', 'police', 'crime', 'victim',\n",
    "            \n",
    "           \n",
    "            'block', 'freeze', 'lock', 'stop', 'cancel', 'protect', \n",
    "            \n",
    "           \n",
    "            'didn\\'t make', 'did not make', 'wasn\\'t me', 'was not me',\n",
    "            'recognise', 'recognize', 'unknown', 'unfamiliar',\n",
    "            \n",
    "            \n",
    "            'details', 'pin', 'cvv', 'information', 'data', 'security',\n",
    "            'numbers', 'copied', 'access', \n",
    "            \n",
    "            \n",
    "            'someone',    \n",
    "            'improperly', \n",
    "            'child',      \n",
    "            'son', 'daughter', \n",
    "            'used'        \n",
    "        ],\n",
    "        'lost_or_stolen_card': [\n",
    "            'stolen', 'lost', 'robbed', 'missing', 'dropped', 'gone', \n",
    "            'thief', 'theft', 'wallet', 'purse', 'bag'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for intent, keywords in risk_map.items():\n",
    "        for word in keywords:\n",
    "            if word in text_lower:\n",
    "                try:\n",
    "                    return label_encoder.transform([intent])[0]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "    model.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbf9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Calculating Final Score...\")\n",
    "risk_indices = label_encoder.transform(['compromised_card'])\n",
    "risk_mask = (y_test == risk_indices[0]) \n",
    "total = np.sum(risk_mask)\n",
    "correct = 0\n",
    "\n",
    "print(\"--- Remaining Missed Cases ---\")\n",
    "for text in test_df[risk_mask]['text']:\n",
    "    if hybrid_predict(text) == risk_indices[0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f\" STILL MISSED: '{text}'\")\n",
    "\n",
    "print(f\"\\n Final Recall Score: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ae603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b584cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"notebooks\" in base_dir:\n",
    "    models_dir = os.path.join(base_dir, \"..\", \"models\")\n",
    "else:\n",
    "    models_dir = os.path.join(base_dir, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef01dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(models_dir, \"label_encoder.pkl\")\n",
    "joblib.dump(label_encoder, save_path)\n",
    "\n",
    "print(f\" Portable Save Successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b273d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
